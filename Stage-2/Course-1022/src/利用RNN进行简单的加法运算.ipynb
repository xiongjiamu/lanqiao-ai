{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01631747",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>目录<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#神经网络层的函数实现\" data-toc-modified-id=\"神经网络层的函数实现-1\">神经网络层的函数实现</a></span></li><li><span><a href=\"#神经网络初始化\" data-toc-modified-id=\"神经网络初始化-2\">神经网络初始化</a></span><ul class=\"toc-item\"><li><span><a href=\"#网络初始化\" data-toc-modified-id=\"网络初始化-2.1\">网络初始化</a></span></li><li><span><a href=\"#训练数据集初始化\" data-toc-modified-id=\"训练数据集初始化-2.2\">训练数据集初始化</a></span></li></ul></li><li><span><a href=\"#训练神经网络\" data-toc-modified-id=\"训练神经网络-3\">训练神经网络</a></span><ul class=\"toc-item\"><li><span><a href=\"#设定加法问题\" data-toc-modified-id=\"设定加法问题-3.1\">设定加法问题</a></span></li><li><span><a href=\"#预测值初始化\" data-toc-modified-id=\"预测值初始化-3.2\">预测值初始化</a></span></li><li><span><a href=\"#训练神经网络\" data-toc-modified-id=\"训练神经网络-3.3\">训练神经网络</a></span><ul class=\"toc-item\"><li><span><a href=\"#数据读取\" data-toc-modified-id=\"数据读取-3.3.1\">数据读取</a></span></li><li><span><a href=\"#网络训练\" data-toc-modified-id=\"网络训练-3.3.2\">网络训练</a></span></li><li><span><a href=\"#误差分析\" data-toc-modified-id=\"误差分析-3.3.3\">误差分析</a></span></li></ul></li><li><span><a href=\"#进行反向传播\" data-toc-modified-id=\"进行反向传播-3.4\">进行反向传播</a></span><ul class=\"toc-item\"><li><span><a href=\"#数据检索\" data-toc-modified-id=\"数据检索-3.4.1\">数据检索</a></span></li><li><span><a href=\"#误差计算\" data-toc-modified-id=\"误差计算-3.4.2\">误差计算</a></span></li><li><span><a href=\"#进行权值更新\" data-toc-modified-id=\"进行权值更新-3.4.3\">进行权值更新</a></span></li></ul></li></ul></li><li><span><a href=\"#实验总结\" data-toc-modified-id=\"实验总结-4\">实验总结</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42080642",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T03:42:33.611764Z",
     "start_time": "2021-12-14T03:42:32.398309Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 16, 9\n",
    "plt.rcParams['figure.dpi'] = 96\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa17ef9",
   "metadata": {},
   "source": [
    "## 神经网络层的函数实现\n",
    "\n",
    "在该实验中将会用到两个用于神经网络层的处理函数， $\\sigma$ 函数及其导数。这两个函数用于神经网络层的数据处理， $\\sigma$ 函数即是网络中的激活函数。这个函数中具有一些非常优良的以便神经网络使用的特性，这个函数可以将任意值映射到区间 $(0, 1)$ 中，方便将数值和概率相挂钩。\n",
    "\n",
    "<img align=\"center\" src=\"../img/01.png\">\n",
    "\n",
    "通过这个函数将数据进行非线性化处理。以便于网络的非线性记忆功能。同时该网络的导数也很容易计算，通过一点的输出值即可求得其导数： $\\sigma' = \\sigma(1 - \\sigma)$ 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adadb7ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T03:42:33.627802Z",
     "start_time": "2021-12-14T03:42:33.614823Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def diff_sigmoid(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f9f862",
   "metadata": {},
   "source": [
    "## 神经网络初始化\n",
    "\n",
    "### 网络初始化\n",
    "\n",
    "这里需要将网络的输入层、隐藏层、输出层进行初始化设定，这里我们要预先将神经网络这三层的神经元个数进行设定，以及将网络层与层之间信息交换的权值网络进行初始化设定，同时定义一个和权值网络大小相等的全零网络，用来储存神经网络更新权值的暂存值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4626c4ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T03:42:33.723912Z",
     "start_time": "2021-12-14T03:42:33.630775Z"
    }
   },
   "outputs": [],
   "source": [
    "# 步长，输入层，隐含层，输出层\n",
    "step, input_size, hidden_size, output_size = 0.1, 2, 32, 2\n",
    "\n",
    "### 权重矩阵 ###\n",
    "weight_i2h = 2 * np.random.random((input_size, hidden_size)) - 1\n",
    "weight_h2o = 2 * np.random.random((hidden_size, output_size)) - 1\n",
    "weight_h2h = 2 * np.random.random((hidden_size, hidden_size)) - 1\n",
    "\n",
    "### 权重更新（缓存） ###\n",
    "temp_weight_i2h = np.zeros_like(weight_i2h)\n",
    "temp_weight_h2o = np.zeros_like(weight_h2o)\n",
    "temp_weight_h2h = np.zeros_like(weight_h2h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5233af",
   "metadata": {},
   "source": [
    "这里首先初始化三个随机数值的矩阵，分别代表输入层和隐藏层、隐藏层和输出层、隐藏层和隐藏层之间的连接权值，在之后的操作中，这三个矩阵中的数值不断随着网络的学习进行更改，最终使得网络中的权值能正确的对给定的输入值进行处理。\n",
    "\n",
    "之后网络设定了三个值为零的大小和权值网络相同的矩阵，为的是进行每一次学习的暂时结果的记录。这些矩阵在每一次循环完之后都会将其重置为零矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee4e0de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T02:25:17.029311Z",
     "start_time": "2021-12-14T02:25:17.024331Z"
    }
   },
   "source": [
    "### 训练数据集初始化\n",
    "\n",
    "这里便于神经网络的更精确的计算，将输入神经网络的数值转换成二进制数字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65224407",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T03:42:33.802119Z",
     "start_time": "2021-12-14T03:42:33.727776Z"
    }
   },
   "outputs": [],
   "source": [
    "bin_digits = 8\n",
    "VALUE_MAX = 2**bin_digits\n",
    "binary = np.unpackbits(np.arange(VALUE_MAX, dtype=np.uint8).reshape(-1, 1),\n",
    "                       axis=1)\n",
    "int2bin = {k: v for k, v in enumerate(binary)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4eeeda",
   "metadata": {},
   "source": [
    "## 训练神经网络\n",
    "\n",
    "这里我们将要对网络进行训练，训练的次数越多网络拟合效果越好。在这个实验中我们选定了 20000 次的训练，本小节下面的所有代码，都是在这个 `for` 循环中进行的。\n",
    "\n",
    "```python\n",
    "for i in range(20000):\n",
    "    pass\n",
    "```\n",
    "\n",
    "### 设定加法问题\n",
    "\n",
    "在这一步中，给神经网络出了一个简单的加法问题，目的就是让神经网络通过 20000 次的训练能够学会简单的加法运算，当训练好网络之后，网络进行加减运算时并不是通常意义上的加减，而是通过一步步权值进行处理最终得到正确答案的。这里我们先给出出题代码。\n",
    "\n",
    "```python\n",
    "a_decimal = np.random.randint(MAXnumber / 2)\n",
    "b_decimal = np.random.randint(MAXnumber / 2)\n",
    "c_decimal = a_decimal + b_decimal\n",
    "a = i2b[a_decimal]\n",
    "b = i2b[b_decimal]\n",
    "c = i2b[c_decimal]\n",
    "```\n",
    "\n",
    "首先代码生成了两个随机的数字，并且保证随机数字小于最大数字的一半（ 需要保证两者相加仍然在神经网络的处理范围之内 ）。之后将这两个数字及其加和转化为二进制数字进行储存。\n",
    "\n",
    "### 预测值初始化\n",
    "\n",
    "在这一步中，将进行预测值的初始化，设定几个数组，将得到的预测值、误差值、输出层导数进行储存。为了之后训练和反向传播进行准备。\n",
    "\n",
    "```python\n",
    "# 网络预测值的二进制数组\n",
    "binary = np.zeros_like(c)\n",
    "# 误差值\n",
    "aError = 0\n",
    "#导数值\n",
    "oplayer_der = list()\n",
    "hdlayer_val = list()\n",
    "hdlayer_val.append(np.zeros(hdnumber)) \n",
    "```\n",
    "\n",
    "### 训练神经网络\n",
    "\n",
    "在训练神经网络的过程中首先将数据进行转换，之后数据通过层作用计算权值，并最终将误差进行计算并将权值进行储存以便之后的反向传播运算。在这步中只是做一下网络的训练，层于层之间的网络权值并没有改变。\n",
    "\n",
    "```python\n",
    "for locate in range(bdigit):\n",
    "    X = np.array([[a[bdigit - locate - 1], b[bdigit - locate - 1]]])\n",
    "    Y = np.array([[c[bdigit - locate - 1]]]).T\n",
    "\n",
    "    hdlayer = sigmoid(np.dot(X, neu_i2h) + np.dot(hdlayer_val[-1], neu_h2h))\n",
    "    oplayer = sigmoid(np.dot(hdlayer,neu_h2o))\n",
    "\n",
    "    oplayer_error = Y - oplayer\n",
    "    oplayer_der.append((oplayer_error) * dersigmoid(oplayer))\n",
    "    aError += np.abs(oplayer_error[0])\n",
    "    binary[bdigit - locate - 1] = np.round(oplayer[0][0])\n",
    "    hdlayer_val.append(copy.deepcopy(hdlayer))\n",
    "```\n",
    "\n",
    "#### 数据读取\n",
    "\n",
    "```python\n",
    "X = np.array([[a[bdigit - locate - 1], b[bdigit - locate - 1]]]) #从右向左检索a、b二进制数字\n",
    "Y = np.array([[c[bdigit - locate - 1]]]).T #检索正确数值\n",
    "```\n",
    "\n",
    "这里将二进制数据中的数字从右往左进行读取，并存入到两个数组之中，便于之后的训练。\n",
    "\n",
    "#### 网络训练\n",
    "\n",
    "```python\n",
    "# 隐藏层\n",
    "hdlayer = sigmoid(np.dot(X, neu_i2h) + np.dot(hdlayer_val[-1], neu_h2h))\n",
    "# 输出层\n",
    "oplayer = sigmoid(np.dot(hdlayer, neu_h2o))\n",
    "```\n",
    "\n",
    "这段代码在整个网络中起着至关重要的作用。\n",
    "\n",
    "处理隐藏层的函数是将现在隐藏层的传入信号 $X$ 和输入层与隐藏层的网络权值 `neu_i2h` 进行矩阵相乘，得到现在时刻网络的传入 `np.dot(X,neu_i2h) `。\n",
    "\n",
    "并和上一时刻网络隐藏层的状态 `np.dot(hdlayer_val[-1], neu_h2h)` （ 上一时刻网络状态的导数 `hdlayer_val[-1]` 和隐藏层与隐藏层的网络权值`neu_h2h` 进行矩阵相乘）进行相加。将加和进行函数处理，得到现在隐藏层的值。\n",
    "\n",
    "输出层将网络隐藏层得到的值 `hdlayer` 和隐藏层与输出层的网络权值 `neu_h2o` 进行矩阵相乘，得到网络的输出值。\n",
    "\n",
    "上面两步是网络训练中的关键步骤。请读者结合前文理解清楚。\n",
    "\n",
    "#### 误差分析\n",
    "\n",
    "```python\n",
    "# 计算误差\n",
    "oplayer_error = Y - oplayer # 真实误差\n",
    "oplayer_der.append((oplayer_error) * dersigmoid(oplayer)) # 每时刻导数值\n",
    "aError += np.abs(oplayer_error[0]) # 累加误差的绝对值\n",
    "```\n",
    "\n",
    "在这个步骤中，首先计算出输出层和真是值的误差 $oplayer\\_error$ ，并将每一次训练输出值的导数进行储存，最后将句对误差进行相加。\n",
    "\n",
    "### 进行反向传播\n",
    "\n",
    "到这一步我们得到了神经网络的输出值，下面需要将神经网络进行反向传播来将网络进行进一步的构建。\n",
    "\n",
    "```python\n",
    "Fhdlayer_dels = np.zeros(hdnumber)\n",
    "for locate in range(bdigit):\n",
    "    X = np.array([[a[locate],b[locate]]])\n",
    "    hdlayer = hdlayer_val[-locate - 1]\n",
    "    hdlayer_pre = hdlayer_val[-locate - 2]\n",
    "\n",
    "    oplayer_dels = oplayer_der[-locate - 1]\n",
    "    hdlayer_dels = (Fhdlayer_dels.dot(neu_h2h.T) + oplayer_dels.dot(neu_h2o.T)) * dersigmoid(hdlayer)\n",
    "\n",
    "    neu_h2oN += np.atleast_2d(hdlayer).T.dot(oplayer_dels)\n",
    "    neu_h2hN += np.atleast_2d(hdlayer_pre).T.dot(hdlayer_dels)\n",
    "    neu_i2hN += X.T.dot(hdlayer_dels)\n",
    "    Fhdlayer_dels = hdlayer_dels\n",
    "```\n",
    "\n",
    "#### 数据检索\n",
    "\n",
    "```python\n",
    "X = np.array([[a[locate], b[locate]]]) # 检索数据\n",
    "hdlayer = hdlayer_val[-locate-1] # 从数据中取出当前隐藏层\n",
    "hdlayer_pre = hdlayer_val[-locate-2] # 从数据中取出前一个隐藏层\n",
    "```\n",
    "\n",
    "这里反向将数据进行检索，以便于之后的误差的计算。通过前一个循环得到的隐藏层的数组，取出相应的隐藏层的值准备误差运算。\n",
    "\n",
    "#### 误差计算\n",
    "\n",
    "```python\n",
    "# 输出层误差\n",
    "oplayer_dels = oplayer_der[-locate - 1]\n",
    "# 隐藏层误差\n",
    "hdlayer_dels = (Fhdlayer_dels.dot(neu_h2h.T) + oplayer_dels.dot(neu_h2o.T)) * dersigmoid(hdlayer)\n",
    "```\n",
    "\n",
    "这里将误差进行统计得到输出层和隐藏层的误差，以便权值进行更新。这里得到了两个网络层的误差，通过这两组误差将权值进行更新。\n",
    "\n",
    "#### 进行权值更新\n",
    "\n",
    "```python\n",
    "neu_i2h += neu_i2hN * step\n",
    "neu_h2o += neu_h2oN * step\n",
    "neu_h2h += neu_h2hN * step\n",
    "\n",
    "neu_i2hN *= 0\n",
    "neu_h2oN *= 0\n",
    "neu_h2hN *= 0\n",
    "```\n",
    "\n",
    "得到了所有的值之后，将网络的权值进行更新，即是这一次训练对网络权值造成的影响会在这一步中进行更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebf5820f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T03:42:46.786447Z",
     "start_time": "2021-12-14T03:42:33.804106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1cb0db78e93425d8149d83a34e50240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% 迭代训练\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for i in tqdm(range(20000)):\n",
    "    ### 构造数据集 ###\n",
    "    a_dec = np.random.randint(VALUE_MAX / 2)\n",
    "    b_dec = np.random.randint(VALUE_MAX / 2)\n",
    "    c_dec = a_dec + b_dec\n",
    "    a_bin = int2bin[a_dec]\n",
    "    b_bin = int2bin[b_dec]\n",
    "    c_bin = int2bin[c_dec]\n",
    "\n",
    "    # 预测值\n",
    "    bin_pred = np.zeros_like(c_bin)\n",
    "    # 误差值\n",
    "    error_value = 0\n",
    "    ### 导数 ###\n",
    "    diff_output_layer = []\n",
    "    diff_hidden_layer = [np.zeros(hidden_size)]\n",
    "\n",
    "    ### 训练模型 ###\n",
    "    for index in range(bin_digits):\n",
    "        ### 加载数据 ###\n",
    "        X = np.array(\n",
    "            [[a_bin[bin_digits - index - 1], b_bin[bin_digits - index - 1]]])\n",
    "        Y = np.array([[c_bin[bin_digits - index - 1]]]).T\n",
    "        ### 正向传播 ###\n",
    "        hidden_layer = sigmoid(\n",
    "            np.dot(X, weight_i2h) + np.dot(diff_hidden_layer[-1], weight_h2h))\n",
    "        output_layer = sigmoid(np.dot(hidden_layer, weight_h2o))\n",
    "        ### 误差分析 ###\n",
    "        temp_error = Y - output_layer\n",
    "        diff_output_layer.append(temp_error * diff_sigmoid(temp_error))\n",
    "        error_value += np.abs(temp_error[0])\n",
    "\n",
    "        bin_pred[bin_digits - index - 1] = np.round(output_layer[0][0])\n",
    "        diff_hidden_layer.append(copy.deepcopy(hidden_layer))\n",
    "        pass\n",
    "\n",
    "    ### 反向传播 ###\n",
    "    rev_hidden_layers = np.zeros(hidden_size)\n",
    "    for index in range(bin_digits):\n",
    "        ### 数据检索 ###\n",
    "        X = np.array([[a_bin[index], b_bin[index]]])\n",
    "        hidden_layer = diff_hidden_layer[-index - 1]\n",
    "        pre_hidden_layer = diff_hidden_layer[-index - 2]\n",
    "        ### 计算误差 ###\n",
    "        output_layer_error = diff_output_layer[-index - 1]\n",
    "        hidden_layer_error = (\n",
    "            rev_hidden_layers.dot(weight_h2h.T) +\n",
    "            output_layer_error.dot(weight_h2o.T)) * diff_sigmoid(hidden_layer)\n",
    "\n",
    "        temp_weight_h2o += np.atleast_2d(hidden_layer).T.dot(\n",
    "            output_layer_error)\n",
    "        temp_weight_h2h += np.atleast_2d(pre_hidden_layer).T.dot(\n",
    "            hidden_layer_error)\n",
    "        temp_weight_i2h += X.T.dot(hidden_layer_error)\n",
    "        rev_hidden_layers = hidden_layer_error\n",
    "        pass\n",
    "\n",
    "    ### 权重更新 ###\n",
    "    weight_i2h += temp_weight_i2h * step\n",
    "    weight_h2o += temp_weight_h2o * step\n",
    "    weight_h2h += temp_weight_h2h * step\n",
    "    temp_weight_h2h *= 0\n",
    "    temp_weight_h2o *= 0\n",
    "    temp_weight_i2h *= 0\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b34632e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T03:42:46.818426Z",
     "start_time": "2021-12-14T03:42:46.790428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "误差值: [2. 2.]\n",
      "预测值:[1 1 1 1 1 1 1 1]\n",
      "真实值:[0 1 1 1 1 1 1 0]\n",
      "118 + 8 = 255\n"
     ]
    }
   ],
   "source": [
    "# %% 模型评估\n",
    "print(\"误差值:\", str(error_value))\n",
    "print(\"预测值:\" + str(bin_pred))\n",
    "print(\"真实值:\" + str(c_bin))\n",
    "value = 0\n",
    "for index, x in enumerate(reversed(bin_pred)):\n",
    "    # 将预测值转换为\n",
    "    value += x * pow(2, index)\n",
    "print(str(a_dec) + \" + \" + str(b_dec) + \" = \" + str(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d5b11e",
   "metadata": {},
   "source": [
    "## 实验总结\n",
    "\n",
    "本文对 RNN 网络结构进行简单的加法训练，通过 20000 次训练使得网络具有较好的计算能力，实现整数在 $2^8$ 范围内的加法计算。\n",
    "\n",
    "模型代码逻辑可能存在错误，模型训练结果不理想。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "目录",
   "title_sidebar": "目录",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
