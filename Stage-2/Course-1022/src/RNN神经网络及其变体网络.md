### 循环神经网络

循环网络是一种对序列数据有较强处理能力的网络，类似于蓝桥云课中已有的课程《 [基于卷积神经网络实现图片风格的迁移](https://www.lanqiao.cn/courses/861)》所介绍的 CNN 网络，这两种网路的结构中都存在权值共享的思想。

在网络模型的不同部分进行权值共享使得模型可以扩展到不同样式的样本，正如 CNN 网络中一个确定好的卷积核模板，几乎可以处理任何大小的图片。将图片中分成多个区域，使用同样的卷积核对每一个区域进行处理，最后可以获得非常好的处理结果。

同样的，循环网络使用的类似的模块（形式上类似，之后会进行讲解）对整个序列进行处理，可以将很长的序列进行泛化，得到需要的结果。

这里提到一点，因为循环神经网络可以将序列进行较好的处理，且时间同样也是有序数列，在实际应用中，RNN 循环神经网络对于处理时序数据具有天然的优势。

#### 网络的结构

类似于 CNN 神经网络的卷积核概念，在 RNN 网络中也类有类似的概念“计算图”。

计算图其实质上是用来描述一个函数方法的模块，也等同于一个数据处理“盒子”。

每一个计算图有输入和输出的数据，同时在某一时刻的计算图的数据对下一时刻计算图造成影响。

计算图可以将按照序列进行展开成为一个具有深度的链结构，在这个深度结构中，也构成了参数的共享。

其中最为普遍的系统形式为：

$$
s^{(t)}=f(s^{(t-1)};\theta)
$$

利用这个公式可以将每一个序列的元素进行连接处理，将公式进行展开可以得到类似于下面的形式：

<img title="" src="https://doc.shiyanlou.com/document-uid245411labid7971timestamp1534841760938.png" alt="" width="" data-align="center">

进行展开后可以得到序列的第 $t$ 个节点（或第 $t$ 时刻）的处理结果。在式子中有不变的参数 $\theta$ ，通过保证 $\theta$ 使得整个网络可以进行权值共享，每一阶段通过参数将前一阶段的状态导入。

### RNN 网络

#### 介绍

在前面的讲解中，提到了什么是循环网络，以及循环网络的公式构造，RNN 网络模型即是循环神经网络的表现。相比于前面描述的循环网络，RNN 中加入了其他参数的输入。即在某一个节点或某一时刻 $t$ （以下统称为某一时刻 $t$ ），输入网络的参数还有外界的信号 $x^{(t)}$ ，这样将上式进行重写，有：

$$
s^{(t)}=f(s^{(t-1)},x^{(t)};\theta)
$$

因为网络的参数通过循环不断地传入网络，这一点类似于人思考的过程，RNN 网络可以保持信息的持久性，这一点在其他的神经网络结构中是不具备的。也正是因为这个特征，RNN 网络可以很好的对时序模块进行处理。

在对于网络的理解，也可以理解成为对于同一个网络，每次传入的参数不同。传入参数的一部分是某一时刻 $t$ 的新参数，另一部分是上一时刻 $t-1$ 已经获取到的参数。

#### RNN 拓扑结构

将 RNN 网络函数

$$
s^{(t)}=f(s^{(t-1)},x^{(t)};\theta)
$$

进行展开后可以获得下图所示的链结构：

<img title="" src="https://doc.shiyanlou.com/document-uid245411labid7971timestamp1534844106676.png" alt="" width="" data-align="center">

在图中黄色框标注的部分为网络的一个计算图，也可以将其称作为一个细胞体。同时图中分别用蓝色和橙色标注了两种不同的参数传导方式。先解释一下每一个参数的含义， $x^{(t)}$ 为 $t$ 时刻的输入参数，$h^{(t)}$ 为隐藏层的激活函数，$o^{(t)}$ 为 $t$ 时刻的输出参数， $L^{(t)}$ 为网络的损失值， $y^{(t)}$ 为目标函数。一般情况下使用图中蓝色线标注的迭代方式，当整个网络位数很高并且信息非常多时，可以利用橙色算法进行传参。

通过将网络展开，能够较好的看到对于一个序列 $x^{(t)}$ ，可以使用 RNN 循环神经网络进行处理，网络具有联想能力，可以将之前的信息加以保存，并在之后的循环中进行调用。

#### RNN 的记忆功能

在上图中我们可以看到 RNN 展开链的形式，这里利用实例解释一下 RNN 的记忆功能。我们将 RNN 网络的拓扑图进行简化。

<img title="" src="https://doc.shiyanlou.com/document-uid245411labid7971timestamp1534841814340.png" alt="" width="" data-align="center">

假如需要使用 RNN 循环网络对一段话进行预测，“我是中国人，我爱中国，我的母语是（）”。对于网络来说，现假设“中国人”信息在 $x^{(2)}$ 输入，括号中的内容在 $o^{(n)}$ 输出。这里就涉及到联想功能，如果说是一个训练好的网络，输出信息时网络会根据前文的信息进行判断该点的输出应该为一种语言的名字，并通过之前的输入“中国人”判断此处应该是“汉语”。

但是对于 RNN 网络仍然是存在一些局限性的，输出的信息还有很多依赖的因素。在理想情况下，RNN 网络的输出精度并不受循环次数的增加而降低。但是在实际应用中，当输入关键信息的位置和输出信息的位置之间的节点变得非常长之后，RNN 神经网络会丧失链接到判断输出的关键信息位置的能力。在实际的应用中，对于循环神经网络，人们经常应用的是 RNN 网络的变体，例如 LSTM 网络。在 LSTM 网络中没有因为位置远近而降低输出精度的问题。

### RNN 变体 LSTM 网络

LSTM 网络，即为长短期记忆网络。因为该网络的结构，该网络适合处理序列中间隔和延时较长的事件。在实际生活中，LSTM 网络已经应用在诸多方面，2015年谷歌将其应用在安卓系统语音识别功能中，在苹果手机中 Siri 功能也整合了 LSTM 网络，百度、亚马逊等公司的产品中很多也应用到了 LSTM 网络。

#### LSTM 细胞体结构

在基础 RNN 网络中，之所以不能解决长期依赖问题，是因为 RNN 处理数据的计算图（细胞体）结构简单，只有一个非常简单的结构，比如说只进行一个 $Sigmoid$ 函数的数据处理，这使得网络很难对长时间的信息进行记忆，同时网络对于哪些信息需要舍弃，哪些信息需要保留也无法做出很好的判断。

在 LSTM 网络中，通过将细胞体结构复杂化，在算法中加入了判断信息是否有用的处理器。在网络中加入了三道“门”，分别叫做：遗忘门、输入门、输出门。信息进入网络中，网络会根据一定的规则来判断信息是否有用，有用的信息将加以保留，无用的信息将进行遗忘。下面我们将 LSTM 的一个细胞体以简图形式呈现。

<img title="" src="https://doc.shiyanlou.com/document-uid245411labid7971timestamp1534841852771.png" alt="" width="" data-align="center">

在这里需要解释一下上图中的一些符号。 $\sigma$ 层代表一个输出的权重，表示信息通过的量，其取值范围为 $[0\ 1]$ 。

当取值为 $0$ 时，代表所有信息都不能通过；

当取值为 $1$ 时，代表所有信息都能通过。

网络通过接受前一个细胞体的两个参数 $h^{(t-1)}$ 、 $C^{(t-1)}$ ，以及外界给予的参数 $x^{(t)}$ ，经过三个门的过滤整合将信息传递给下一个细胞体。上图中黑色和蓝色的线是信息在整个网络中信息的传送带，将信息在网络中不断的传输。

在上图两条线合并的地方表示将信息进行合并（蓝线和黑线不相交），在实际处理中，表示将两个向量进行合并。一条线分开成两条线的地方表示将信息进行复制（不需要考虑颜色），黑色方块代表 LSTM 神经网络中的一层，而圆圈代表将信息进行相应的处理。

细胞体中颜色的区分是为了让读者更好理解三个门的概念。第一个门用橙色线进行标注为忘记门；第二个门用绿色线和黄色线标注为输入门；第三个门是用灰色线进行标注为输出门。下面将详细讲三个门结构进行讲解。

#### 结构分步讲解

LSTM 网络涉及到三个门的操作，将三个门分开进行讨论会使得该网络便于读者进行理解。

#### 遗忘门

遗忘门为网络中的第一个门，网络会对信息进行判断，决定信息的弃留。为什么遗忘门作为网络的第一个门，是因为需要将网络信息进行过滤，放在之后容易对新加入的信息进行错误处理。

在遗忘门中信息经过一个 $Sigmoid$ 函数处理得到新的信息 $f_{1}^{(t)}$ ，这个网络层决定了信息的保存还是丢弃。

$$
f_{1}^{(t)}=\sigma(W_{f_{1}}\times[h^{(t-1)},x^{(t)}]+b_{f_{1}})
$$

应用到实际应用中，细胞体中可能含有的是“小明”这个人名，我们很希望使用它来预测之后的词语，网络中可能已经获取的信息是“忧伤的”，但经过网络处理中后该词需要进行更新，遗忘门就会将该词进行过滤。

#### 输入门

输入门分为两步进行，首先绿色线的部分需要选择什么值需要进行更新，之后黄色线的部分是确定将什么值进行传输。这两步中可以用如下公式进行解释。

绿色线部分： 

$$
f_{2}^{(t)}=\sigma(W_{f_{2}}\times[h^{(t-1)},x^{(t)}]+b_{f_{2}})
$$

黄色线部分： 

$$
f_{3}^{(t)}=tanh(W_{f_{3}}\times[h^{(t-1)},x^{(t)}]+b_{f_{3}})
$$

现在我们已经将网络信息中需要更新的旧信息予以遗忘，新信息定位好并获取其内容，之后就是将信息进行整合，更新细胞体状态，并且这里得到细胞体的一部分输出：

$$
C^{(t)}=C^{(t-1)}\times f_{1}^{(t)}+f_{2}^{(t)}\times f_{3}^{(t)}
$$

#### 输出门

最后是将信息进行输出，输出门所采用的信息是将细胞状态进行处理并和初始信息整合后进行输出。

$$
f_{4}^{(t)}=\sigma(W_{f_{4}}\times[h^{(t-1)},x^{(t)}]+b_{f_{4}})
$$

$$
h^{(t)}=f_{4}^{(t)}\times tanh(C^{(t)})
$$

通过最后输出门的处理，将细胞信息进行了选择，输出了我们需要的那部分信息，比如按照之前的语料信息，“小明”现在的状态并不是“忧伤的”而是“快乐的” 。这时网络的输出会将“快乐的”这个信息进行输出。

### RNN 其他形式变体介绍

在实际应用中，会使用到很多 LSTM 网络的变体，通过这些变体也演化出非常多的应用。

#### Peephole 网络

这个变体是由 Gers 和 Schmidhuber 在 2000 年提出的，该网络将细胞体的状态作为网络神经层的一部分输入。整合了细胞状态的输入，使得网络判断可依据的信息变多，这种优化的方法也称作是窥孔优化。

<img title="" src="https://doc.shiyanlou.com/document-uid245411labid7971timestamp1534843939011.png" alt="" width="" data-align="center">

#### Coupled 网络

在这个网络中，细胞状态的遗忘是相对的，遗忘门遗忘了一些信息之后，相应的在输入门就会生成一些信息。未被遗忘的信息将传输到输入门中，有：

$$
f_{2}^{(t)}=1-f_{1}^{(t)}
$$

<img title="" src="https://doc.shiyanlou.com/document-uid245411labid7971timestamp1534844000295.png" alt="" width="" data-align="center">

#### GRU 网络

该网络是简化版的 LSTM 网络，在这个网络中将细胞状态的概念去掉，通过一条输出链在每时刻和输入信息进行整合作为输出并传输到下一时刻。正因为这样的结构，使得 GRU 网络收敛的时间和需要的信息量的要求上都优于 LSTM 网络。 

<img title="" src="https://doc.shiyanlou.com/document-uid245411labid7971timestamp1534842491259.png" alt="" width="" data-align="center">

### 实验总结

本文大体介绍了 RNN 算法及其变体算法 LSTM 算法，通过讲解循环网络的结构使得读者能够初步理解 RNN 网络的运行模式，对 RNN 算法进行了解。之后将 RNN 算法的变体 LSTM 算法进行讲解。通过对 LSTM 细胞体结构的讲解，使用户能理解 LSTM 网络中三个门的概念，然后分步进行每个门的运算展示。最后希望读者能够了解其他 LSTM 网络的变体。
